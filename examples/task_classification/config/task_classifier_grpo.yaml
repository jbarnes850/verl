# Task Classification GRPO Configuration
# Optimized for binary classification with VLM judge verification

# Model configuration
model:
  path: Qwen/Qwen2.5-VL-3B-Instruct
  device_map: auto
  enable_gradient_checkpointing: true
  use_remove_padding: true

# Data configuration  
data:
  max_prompt_length: 256      # Task description + image
  max_response_length: 16     # Just classification token
  train_batch_size: 32
  val_batch_size: 64
  image_key: images
  filter_overlong_prompts: true
  truncation: 'error'

# Algorithm configuration
algorithm:
  adv_estimator: grpo
  use_kl_in_reward: false
  reward_type: hybrid
  reward_weights:
    binary_classification: 0.5  # Core binary reward
    vlm_judge: 0.5             # VLM judge verification
    # visual_verifier: 0.0     # Disabled initially for simplicity

# Actor-Rollout-Reference configuration
actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-VL-3B-Instruct
    use_remove_padding: true
    enable_gradient_checkpointing: true
  
  actor:
    optim:
      lr: 1e-6
    ppo_mini_batch_size: 32
    ppo_micro_batch_size_per_gpu: 2
    use_kl_loss: true
    kl_loss_coef: 0.01
    kl_loss_type: low_var_kl
    entropy_coeff: 0.01  # Prevent mode collapse
    use_torch_compile: false
    fsdp_config:
      param_offload: false
      optimizer_offload: false
  
  rollout:
    name: vllm
    n: 5  # Group sampling for GRPO
    log_prob_micro_batch_size_per_gpu: 4
    tensor_model_parallel_size: 1  # Adjusted for 3B model
    engine_kwargs:
      vllm:
        disable_mm_preprocessor_cache: true
    gpu_memory_utilization: 0.6  # Increased for 3B model
    enable_chunked_prefill: false
    enforce_eager: false
    free_cache_engine: false
  
  ref:
    log_prob_micro_batch_size_per_gpu: 4
    fsdp_config:
      param_offload: true

# Judge configuration
judge:
  enable: true
  model_path: Qwen/Qwen2.5-VL-72B-Instruct
  verification_mode: preference
  cache_judgments: true
  gpu_memory_utilization: 0.8
  tensor_model_parallel_size: 4  # For 72B model

# Training configuration
trainer:
  total_epochs: 10
  save_freq: 100
  eval_freq: 50
  critic_warmup: 0
  logger: ['console']
  project_name: 'verl_task_classification'
  experiment_name: 'qwen2_5_vl_3b_binary_classifier'
  n_gpus_per_node: 4
  nnodes: 1
  test_freq: -1

# Semi-online learning configuration
semi_online_learning:
  enable: true
  sync_steps: 10  # Sync generator with trainer every 10 steps
  feedback_buffer_size: 10000
  synthetic_variants_per_feedback: 10
  update_frequency: 100

# Reward configuration
reward:
  binary_reward_scale: 1.0
  judge_reward_scale: 1.0
  confidence_threshold: 0.8  # Minimum confidence for judge agreement