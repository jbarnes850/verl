# SEC Curriculum Training Config for VERL
# Integrates Self-Evolving Curriculum with VERL's GRPO implementation

# dataset config
data:
  # SEC Curriculum Configuration
  use_sec_curriculum: True
  sec_alpha: 0.3  # SEC paper TD(0) learning rate
  sec_tau: 0.5    # SEC paper softmax temperature
  
  # CRMArena data configuration
  use_crmarena: true
  dataset_name: "Salesforce/CRMArenaPro"
  train_tasks: ["case_routing", "lead_routing", "lead_qualification", "knowledge_qa", "sales_insight_mining", "wrong_stage_rectification", "activity_priority", "named_entity_disambiguation", "handle_time", "transfer_count", "top_issue_identification", "monthly_trend_analysis", "best_region_identification", "sales_amount_understanding", "sales_cycle_understanding", "conversion_rate_comprehension"]
  test_tasks: ["invalid_config", "policy_violation_identification", "quote_approval"]
  train_split_ratio: 0.75
  cache_dir: "./data/verl_format"
  
  # Data paths (will be auto-generated from CRMArena data)
  train_files: ["./data/verl_format/train.parquet"]
  val_files: ["./data/verl_format/val.parquet"]
  
  prompt_key: prompt
  max_prompt_length: 1024
  max_response_length: 4096
  
  # SEC paper requirement
  train_batch_size: 256
  val_batch_size: 32
  
  shuffle: False  # SEC curriculum handles sampling

# model config  
actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-3B
    enable_gradient_checkpointing: True
    
  rollout:
    name: vllm
    mode: sync
    # SEC paper requirement: 8 rollouts
    n: 8
    temperature: 0.7
    top_p: 0.9
    top_k: -1
    
    # Response generation settings
    prompt_length: 1024
    response_length: 4096
    
    # vLLM settings for 2xH100
    dtype: bfloat16
    gpu_memory_utilization: 0.5
    ignore_eos: false
    enforce_eager: true
    free_cache_engine: true
    load_format: dummy_dtensor
    
    # 2xH100 tensor parallelism  
    tensor_model_parallel_size: 2
    max_num_batched_tokens: 8192
    max_num_seqs: 1024
    
  actor:
    # Required: FSDP strategy for 2xH100 setup
    strategy: fsdp
    
    # Required: PPO batch configuration
    ppo_mini_batch_size: 256
    ppo_micro_batch_size_per_gpu: null
    ppo_max_token_len_per_gpu: 16384
    
    # Required: PPO algorithm parameters
    clip_ratio: 0.2
    ppo_epochs: 1
    entropy_coeff: 0
    
    # SEC paper: no KL loss in actor (GRPO uses it in algorithm section)
    use_kl_loss: false
    kl_loss_coef: 0.0
    
    # Required: gradient and loss settings
    grad_clip: 1.0
    loss_agg_mode: token-mean
    shuffle: false
    
    optim:
      # SEC paper requirement
      lr: 0.000001
      betas: [0.9, 0.99]
      eps: 0.00000001
      weight_decay: 0.01
      
    # Required: FSDP configuration
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: false
      optimizer_offload: false
      reshard_after_forward: true
      fsdp_size: -1

# critic config (required by VERL even for GRPO)
critic:
  strategy: fsdp
  rollout_n: 8
  
  model:
    path: Qwen/Qwen2.5-3B
    enable_gradient_checkpointing: true
    trust_remote_code: false
    
  optim:
    lr: 1e-5
    weight_decay: 0.01
    
  ppo_mini_batch_size: 256
  ppo_epochs: 1
  grad_clip: 1.0
  loss_agg_mode: token-mean
  
  fsdp_config:
    wrap_policy:
      min_num_params: 0
    param_offload: false
    reshard_after_forward: true
    fsdp_size: -1

# algorithm config  
algorithm:
  # SEC paper uses GRPO
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: true
  
  # SEC paper: no KL penalty in reward
  use_kl_in_reward: false
  kl_penalty: kl
  
  gamma: 1.0
  lam: 1.0

# reward model config (required by VERL)
reward_model:
  enable: false
  strategy: fsdp

# trainer config
trainer:
  project_name: sec_curriculum_verl
  experiment_name: sec_grpo_integration
  total_epochs: 1
  max_epochs: 1
  save_freq: 50
  test_freq: 25
  n_gpus_per_node: 2
  nnodes: 1

# Ray initialization
ray_init:
  num_cpus: 16